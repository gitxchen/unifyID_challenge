{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data\n",
    "\n",
    "The sensors were sampled at 50Hz, and we want to classify at 1 second resolution, so each training example has 50 timesteps. One problem was that the 'sitting' and 'standing' activity timestamps were corrupted and all displayed the same time. However, going over the rest of the data, it was clear that most 50-sample slices were already chronologically ordered, so assuming that the sampe slices for the 'sitting' and 'standing' activities were already chronologically ordered as well is not a far stretch. Concretely, only 32 instances arise where, in a 50-sample slice, timestamp(A)>timestamp(B) when index(A)<index(B). Therefore, we assume that the samples for the 'sitting' and 'standing' activities are already chronologically ordered.\n",
    "\n",
    "We don't preprocess the features in any way, but we do add additional features - namely the magnitude of the vectors generated by each of the sensors (root of the sum of squares of the x,y, and z values). This idea was adopted from the original paper that accompanied this dataset.\n",
    "\n",
    "To create the train, val, and test sets, we split the data by activity label and perform stratified sampling to ensure a good train/val/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data\n",
      "skipped 1302 samples\n",
      "Standing examples: train 1521 val 217 test 434\n",
      "Walking examples: train 1533 val 219 test 438\n",
      "Upstairs examples: train 1025 val 146 test 293\n",
      "Sitting examples: train 949 val 135 test 271\n",
      "Running examples: train 1549 val 221 test 442\n",
      "Downstairs examples: train 1532 val 218 test 437\n",
      "8109 training samples\n",
      "1161 val samples\n",
      "2319 test samples\n"
     ]
    }
   ],
   "source": [
    "print 'Importing data'\n",
    "data_dir = 'Activity_Recognition_DataSet/'\n",
    "sources = ['Arm','Belt','Pocket','Wrist']\n",
    "keys = { 'Standing': 0, 'Sitting': 1, 'Downstairs': 2, 'Upstairs': 3, 'Running': 4, 'Walking': 5}\n",
    "data = [ [] for i in range( len( keys ) )]\n",
    "num_classes = len(keys)\n",
    "time_steps = 50\n",
    "skipped_samples = 0\n",
    "for idx in range( len( sources ) ):\n",
    "  record = pd.read_excel( data_dir + sources[idx] + '.xlsx')\n",
    "  record.sort_values( 'Time_Stamp', inplace= True)\n",
    "  for idx2 in range( len( sources ) ):\n",
    "    record[ sources[idx2] ] = 1 if idx2 == idx else 0\n",
    "\n",
    "  record['Activity_Label'] = record['Activity_Label'].apply( lambda x: keys[x])\n",
    "  labels = record['Activity_Label']\n",
    "  record.drop(labels=['Activity_Label'], axis=1,inplace = True)\n",
    "  record['A'] = ( record['Ax']**2 + record['Ay']**2 + record['Az']**2 ) **0.5\n",
    "  record['G'] = ( record['Gx']**2 + record['Gy']**2 + record['Gz']**2 ) **0.5\n",
    "  record['M'] = ( record['Mx']**2 + record['My']**2 + record['Mz']**2 ) **0.5\n",
    "\n",
    "  for sample_idx in range(0, len(labels), time_steps):\n",
    "    if sample_idx + time_steps - 1 < len(labels):\n",
    "      time_diff = record['Time_Stamp'][sample_idx+time_steps - 1] - record['Time_Stamp'][sample_idx]\n",
    "      if time_diff <= time_steps/50.0 * 1000 \\\n",
    "          and time_diff >= 0 \\\n",
    "          and labels[sample_idx: sample_idx + time_steps].min() == labels[sample_idx: sample_idx + time_steps].max():\n",
    "        sample_x = record.iloc[sample_idx: sample_idx + time_steps]\n",
    "        data[ labels[sample_idx] ].append( ( sample_x.as_matrix(), labels[sample_idx] ) )\n",
    "      else:\n",
    "        skipped_samples = skipped_samples + 1\n",
    "    else:\n",
    "      skipped_samples = skipped_samples + 1\n",
    "\n",
    "print 'skipped ' + str(skipped_samples) + ' samples'\n",
    "\n",
    "train_set, val_set, test_set = [],[],[]\n",
    "\n",
    "for idx in range( len(data) ):\n",
    "  label_data = data[idx]\n",
    "  random.shuffle(label_data)\n",
    "  train_set.extend( label_data[0: int(0.7*len(label_data))] )\n",
    "  val_set.extend( label_data[int(0.7*len(label_data)): int(0.8*len(label_data))] )\n",
    "  test_set.extend( label_data[int(0.8*len(label_data)):] )\n",
    "\n",
    "  print keys.keys()[idx] + ' examples: train %d val %d test %d'%(int(0.7*len(label_data)),\n",
    "                                                                  int(0.1*len(label_data)),\n",
    "                                                                  int(0.2*len(label_data)))\n",
    "random.shuffle(train_set)\n",
    "random.shuffle(val_set)\n",
    "random.shuffle(test_set)\n",
    "train_X, train_Y = zip( *train_set )\n",
    "val_X, val_Y = zip( *val_set )\n",
    "test_X, test_Y = zip( *test_set )\n",
    "\n",
    "train_X, train_Y = np.stack(train_X), np.array(train_Y)\n",
    "val_X, val_Y = np.stack(val_X), np.array(val_Y)\n",
    "test_X, test_Y = np.stack(test_X), np.array(test_Y)\n",
    "\n",
    "print '%d training samples'%(train_X.shape[0])\n",
    "print '%d val samples'%(val_X.shape[0])\n",
    "print '%d test samples'%(test_X.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "In most of the literature in this area, we see that neural network techniques come out on top relative to other classifiers. Therefore we will attempt to use more recent techniques - specifically, we will use a LSTM layer followed by two fully-connected layers. Additionally, we add batch normalization layers at the input and between the two fully-connected layers to normalize the input and hidden state. With this setup, the model achieves a test accuracy of about 87%."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print 'Building Model'\n",
    "input_var = tf.placeholder(tf.float32, shape = [None,time_steps,train_X.shape[2]-1])\n",
    "true_labels = tf.placeholder(tf.int32)\n",
    "learning_rate = 0.05\n",
    "\n",
    "lstm_dim = 100\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_dim)\n",
    "\n",
    "rnn_output, _ = tf.nn.dynamic_rnn(lstm_cell, tf.contrib.layers.batch_norm(input_var), dtype = tf.float32)\n",
    "rnn_output = rnn_output[:,-1]\n",
    "\n",
    "fc_w_1 = tf.get_variable('fc_w_1', shape = [lstm_dim, lstm_dim] )\n",
    "fc_w_2 = tf.get_variable('fc_w_2', shape = [lstm_dim, num_classes])\n",
    "\n",
    "fc_b_1 = tf.get_variable('fc_b_1', shape = [lstm_dim] )\n",
    "fc_b_2 = tf.get_variable('fc_b_2', shape = [num_classes] )\n",
    "\n",
    "fc_1 = tf.add( tf.matmul( rnn_output, fc_w_1), fc_b_1)\n",
    "fc_1 = tf.nn.relu(fc_1)\n",
    "fc_1 = tf.contrib.layers.batch_norm(fc_1)\n",
    "output = tf.add( tf.matmul( fc_1, fc_w_2), fc_b_2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(output,true_labels))\n",
    "train_step = tf.train.AdadeltaOptimizer(learning_rate).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.to_int32(tf.argmax( output, 1)), true_labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print 'Training'\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for epoch in range(200):\n",
    "    for batch_idx in range(0,train_X.shape[0], 50):\n",
    "      inX = train_X[batch_idx: batch_idx + 50] if batch_idx + 50 < train_X.shape[0] else train_X[batch_idx:]\n",
    "      inY = train_Y[batch_idx: batch_idx + 50] if batch_idx + 50 < train_Y.shape[0] else train_Y[batch_idx:]\n",
    "      inX = inX[:,:,1:]\n",
    "\n",
    "      train_step.run( feed_dict = {input_var: inX, true_labels: inY} )\n",
    "    print 'Epoch: %d Val acc: %.2f'%(epoch, accuracy.eval( feed_dict = {input_var: val_X[:,:,1:], true_labels: val_Y} ) )\n",
    "  print 'Test acc: %.2f'%(accuracy.eval( feed_dict = {input_var: test_X[:,:,1:], true_labels: test_Y} ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
